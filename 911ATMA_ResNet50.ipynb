{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE: make sure to fully annotate any changes you've made that way we can keep track of what has changed. Code no longer used should be commented out, not deleted.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 911-Crime detection using ResNet50 Model\n",
    "\n",
    "\n",
    "###### By Project ATMA Team"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "For this model, we decided to use the ResNet50 (Deep Residual Networks with 50 layers) to help us determine whether a given short video input can be classified as an active crime.\n",
    "\n",
    "In this python notebook, we will explore the use of Transfer Values (features) in order to re-train a model's trained weights to recognize crimes without the need to train the full model from scratch (which usually requires a large amount of data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### More about ResNet50\n",
    "\n",
    "ResNet has an exotic architecture also called \"network on network architecture\". Such micro-architecture modules refer to the building blocks that make up the network. Together with the standard layers, a macho-architecture is formed and \"residual learning\" is introduced. Ever since introduced by He et al., ResNets have demostrated that deep networks can be trained with a standard SGD (Stochastic Gradient Descent) optimizer.\n",
    "\n",
    "Writen By Kartik Ordugo,\n",
    "https://www.quora.com/What-is-the-deep-neural-network-known-as-%E2%80%9CResNet-50%E2%80%9D\n",
    "\n",
    "\"Deep convolutional neural networks have led to a series of breakthroughs for image classification. Many other visual recognition tasks have also greatly benefited from very deep models. So, over the years there is a trend to go more deeper, to solve more complex tasks and to also increase /improve the classification/recognition accuracy. But, as we go deeper; the training of neural network becomes difficult and also the accuracy starts saturating and then degrades also. Residual Learning tries to solve both these problems.\n",
    "\n",
    "In general, in a deep convolutional neural network, several layers are stacked and are trained to the task at hand. The network learns several low/mid/high level features at the end of its layers. In residual learning, instead of trying to learn some features, we try to learn some residual. Residual can be simply understood as subtraction of feature learned from input of that layer. ResNet does this using shortcut connections (directly connecting input of nth layer to some (n+x)th layer. It has proved that training this form of networks is easier than training simple deep convolutional neural networks and also the problem of degrading accuracy is resolved.\"\n",
    "\n",
    "ResNets take activations from one layer and feed it into another layer much deeper in the network. This is called \"Skip connections\". they work because the identity function is easy for residual blocks to learn, as the same input is used and transferred into a deeper layer and in the case that the weights/bias fails to change the input (by applying weight/bias decay), the relu goes back to the skipped input. Thereby learning the identity function.\n",
    "\n",
    "* Deep Residual Learning for Image Recognition by He et al.\n",
    "    - https://arxiv.org/abs/1512.03385\n",
    "* Identity Mappings in Deep Residual Networks by He et al.\n",
    "    - https://arxiv.org/abs/1603.05027\n",
    "* Youtube videos explaining Residual Networks by Andrew Ng\n",
    "    - ResNets https://www.youtube.com/watch?time_continue=1&v=K0uoBKBQ1gA\n",
    "    - Why ResNets work? https://www.youtube.com/watch?v=GSsKdtoatm8\n",
    "    - Network in Network architecture https://www.youtube.com/watch?v=9EZVpLTPGz8\n",
    "\n",
    "###### Below is an image of a residual module (Left) next to an updated residual module (Right) that uses pre-activation.\n",
    "\n",
    "Demostrated in 2016 in a follow up paper (also given above), identity mappings helps the ResNets achieve higher accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"images/imagenet_resnet_residual_identity.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modules to display images\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML, display\n",
    "# Display two images\n",
    "# display(HTML(\"<table><tr><td><img src='images/imagenet_resnet_residual.png'></td><td><img src='images/imagenet_resnet_residual_identity.png'></td></tr></table>\"))\n",
    "Image(url= \"images/imagenet_resnet_residual_identity.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### ResNet50 Architecture Graph\n",
    "\n",
    "Click the link below for a detailed graph of the ResNet50 architecture\n",
    "\n",
    "http://ethereon.github.io/netscope/#/gist/db945b393d40bfa26006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data directory structure"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We will be using the Dogs vs Cats dataset from Kaggle because CIFAR-10 and MNIST have relatively low pixelation images.\n",
    "\n",
    "Dogs vs Cats data from Kaggle: https://www.kaggle.com/c/dogs-vs-cats/data\n",
    "\n",
    "Example: Dogs vs Cats (Directory Structure)\n",
    "\n",
    "data/\n",
    "    train/\n",
    "        dogs/\n",
    "            dog001.jpg\n",
    "            dog002.jpg\n",
    "            ...\n",
    "        cats/\n",
    "            cat001.jpg\n",
    "            cat002.jpg\n",
    "            ...\n",
    "    validation/\n",
    "        dogs/\n",
    "            dog001.jpg\n",
    "            dog002.jpg\n",
    "            ...\n",
    "        cats/\n",
    "            cat001.jpg\n",
    "            cat002.jpg\n",
    "            ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### We can also collect videos of spoons, forks, and knives and use that as our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ImageNet\n",
    "\n",
    "What is ImageNet?\n",
    "\n",
    "ImageNet is formally a project aimed at (manually) labeling and categorizing images into almost 22,000 separate object categories for the purpose of computer vision research.\n",
    "\n",
    "\n",
    "The goal of the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) is to train a model that can correctly classify an input image into 1,000 separate object categories. Models are trained on ~1.2 million training images with another 50,000 images for validation and 100,000 images for testing.\n",
    "\n",
    "These 1,000 image categories represent object classes that we encounter in our day-to-day lives, such as species of dogs, cats, various household objects, vehicle types, and much more. You can find the full list of object categories in the ILSVRC challenge here.\n",
    "http://image-net.org/challenges/LSVRC/2014/browse-synsets\n",
    "\n",
    "###### This dataset is what the ResNet50 model is trained on. Therefore, it is good to know the classification labels from that dataset in order for us to work with the pre-trained transfer values.\n",
    "\n",
    "When it comes to image classification, the ImageNet challenge is the de facto benchmark for computer vision classification algorithms â€” and the leaderboard for this challenge has been dominated by Convolutional Neural Networks and deep learning techniques since 2012. The state-of-the-art pre-trained networks included in the Keras core library represent some of the highest performing Convolutional Neural Networks on the ImageNet challenge over the past few years. These networks also demonstrate a strong ability to generalize to images outside the ImageNet dataset via transfer learning, such as feature extraction and fine-tuning.\n",
    "\n",
    "###### ImageNet Classification classes\n",
    "\n",
    "* Letter opener, paper knife, paperknife - 1170 images\n",
    "* Assault rifle, assault gun - 1172 images\n",
    "* Revolver, six-gun, six-shooter - 1223 images\n",
    "* Sweatshirt - 1174 images\n",
    "* Jersey, T-shirt, tee shirt - 1331 images\n",
    "0. ALL SYNSETS BELOW, NEED TO HAVE NUMBER OF IMAGES.\n",
    "1. revolver, six-gun, six-shooter\n",
    "2. hatchet\n",
    "3. cleaver, meat cleaver, chopper\n",
    "4. guillotine\n",
    "6. rifle\n",
    "7. lighter, light, igniter, ignitor\n",
    "8. holster\n",
    "9. matchstick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks\n",
    "* Find a way to connect GCP VM to Git\n",
    "* Create a python module that converts videos to frames (recommended to use ffmpeg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bugs / Issues\n",
    "## 1. \n",
    "test_generator = test_genFunction.flow_from_directory(test_dir_path,\n",
    "                                                 target_size=(img_width, img_height),\n",
    "                                                 batch_size=1,\n",
    "                                                 shuffle=False)\n",
    "                                                 \n",
    "Output: Found 9 images belonging to 3 classes.\n",
    "###### However, there are only two classes inside the 'test' directory. The issue does not arise with the training or the validation directory. What might the problem be?\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "###### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nfrom __future__ import division\\n\\nimport six\\nfrom keras.models import Model\\nfrom keras.layers import (\\n    Input,\\n    Activation,\\n    Dense,\\n    Flatten\\n)\\nfrom keras.layers.convolutional import (\\n    Conv2D,\\n    MaxPooling2D,\\n    AveragePooling2D\\n)\\nfrom keras.layers.merge import add\\nfrom keras.layers.normalization import BatchNormalization\\nfrom keras.regularizers import l2\\nfrom keras import backend as K\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Utilities\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import pandas\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.applications import *\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# preprocess_input is a fn used to process the image for ResNet50\n",
    "# decode_predictions gives the top predictions (given as an arg.)\n",
    "# Given as a tuple (class, description, probability)\n",
    "# preds = model.predict(x)\n",
    "# print('Predicted:', decode_predictions(preds, top=3)[0])\n",
    "# Predicted: [(u'n02504013', u'Indian_elephant', 0.82658225), ... (other two predictions)\n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "\n",
    "\n",
    "'''\n",
    "# \n",
    "from keras.applications import ResNet50\n",
    "from keras.preprocessing import image\n",
    "--- to use in : \n",
    "img_path = 'elephant.jpg'\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "'''\n",
    "\n",
    "'''\n",
    "from __future__ import division\n",
    "\n",
    "import six\n",
    "from keras.models import Model\n",
    "from keras.layers import (\n",
    "    Input,\n",
    "    Activation,\n",
    "    Dense,\n",
    "    Flatten\n",
    ")\n",
    "from keras.layers.convolutional import (\n",
    "    Conv2D,\n",
    "    MaxPooling2D,\n",
    "    AveragePooling2D\n",
    ")\n",
    "from keras.layers.merge import add\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.11.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check version of anything. In this case, we check tensorflow\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "last_block_layer_of_base_model = 126\n",
    "img_width, img_height = 299, 299 # default parameters for ResNet50 is 224x224\n",
    "num_channels = 3 # 3 color channels for the frames (RBG)\n",
    "batch_size = 32 # we can try 4,8,32,64,128,256,..\n",
    "num_epochs = 50 # number of iterations the algorithm gets trained\n",
    "learning_rate = 0.045 # for sgd optimizer\n",
    "learning_rate_decay = 0.94 # every two seconds\n",
    "momentum = 0.9 # momentum used for the sgd optimizer\n",
    "transformation_ratio = .05 # how aggressive will the data augmentation/transformation be"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Loading the model (Incomplete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ivargaswhs88/anaconda3/lib/python3.5/site-packages/keras_applications/resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
      "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
     ]
    }
   ],
   "source": [
    "base_model = keras.applications.resnet50.ResNet50(include_top=False, weights='imagenet')\n",
    "# ADD \"input_shape=(img_width, img_height, num_channels)\" when we want to specify the shape of the frame input\n",
    "# The default input size for this model is 224x224.\n",
    "\n",
    "# ARGS:\n",
    "# include_top = False -> we will not get the last two fully connected layers\n",
    "# weights = 'imagenet' -> we will get the weights of the model after being trained by the given dataset\n",
    "\n",
    "# Show the model's architecture\n",
    "# base_model.summary()\n",
    "\n",
    "# the output shape of the Base model.\n",
    "# base_model.output_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Model Completion with Keras Functional model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Finishing up the architecture\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x) # add a pooling layer. turning 2048 features into 1024\n",
    "x = Dense(1024, activation='relu')(x) # a fc player with relu non-linear activation\n",
    "predictions = Dense(num_classes, activation='softmax')(x) # a logistic layer with the number of classes and softmax to normalize the outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining the model start and end points\n",
    "model = Model(inputs=base_model.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model Compilation\n",
    "model.compile(optimizer='nadam',\n",
    "              loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Acquiring the data from the zip file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pwd\n",
    "# ^ gets us the home directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# Unzip data files into directory path given\\nimport zipfile\\n# 'pwd' gets home folder where notebook opened. Very useful to get paths\\nzip_ref = zipfile.ZipFile('/home/ivargaswhs88/sdata.zip','r')\\n# extracts what is in the zip file, which is already a folder called sdata\\n# so there is no need to create a new directory\\nzip_ref.extractall('/home/ivargaswhs88')\\nzip_ref.close()\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Unzip data files into directory path given\n",
    "import zipfile\n",
    "# 'pwd' gets home folder where notebook opened. Very useful to get paths\n",
    "zip_ref = zipfile.ZipFile('/home/ivargaswhs88/sdata.zip','r')\n",
    "# extracts what is in the zip file, which is already a folder called sdata\n",
    "# so there is no need to create a new directory\n",
    "zip_ref.extractall('/home/ivargaswhs88')\n",
    "zip_ref.close()'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Acquiring the data path directories for each set (training, validation, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_dir_path = os.path.abspath('/home/ivargaswhs88/sdata')\n",
    "train_dir_path = os.path.join(os.path.abspath(data_dir_path), 'train')\n",
    "validation_dir_path = os.path.join(os.path.abspath(data_dir_path), 'validation')\n",
    "test_dir_path = os.path.join(os.path.abspath(data_dir_path), 'test')\n",
    "\n",
    "# validation for real model we can simply have one full taining set and use a random validation block of close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Preprocessing the Data \n",
    "###### In this case, all arguments are \"false\" so that the data we use for testing purposes isn't messed with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1500 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# function used to randomize the image parameters\n",
    "train_genFunction = ImageDataGenerator(rescale=1. / 255)\n",
    "# data generator that uses above function and applies it to the training files\n",
    "train_generator = train_genFunction.flow_from_directory(train_dir_path,\n",
    "                                                              target_size=(img_width, img_height),\n",
    "                                                              batch_size=batch_size,\n",
    "                                                              color_mode='rgb',\n",
    "                                                              class_mode='categorical',\n",
    "                                                              shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 500 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_genFunction = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "validation_generator = validation_genFunction.flow_from_directory(validation_dir_path,\n",
    "                                                             target_size=(img_width, img_height),\n",
    "                                                             batch_size=batch_size,\n",
    "                                                             color_mode='rgb',          \n",
    "                                                             class_mode='categorical',\n",
    "                                                             shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "test_genFunction = ImageDataGenerator(rescale=1. / 255)\n",
    "test_generator = test_genFunction.flow_from_directory(test_dir_path,\n",
    "                                                     target_size=(img_width, img_height),\n",
    "                                                     batch_size=1,\n",
    "                                                     shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nImporting the knifey dataset used in the Hvass-Labs tutorials (8,9) \\nimport knifey\\nknifey.maybe_download_and_extract()\\nknifey.copy_files()\\ntrain_dir = knifey.train_dir\\ntest_dir = knifey.test_dir'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Importing the knifey dataset used in the Hvass-Labs tutorials (8,9) \n",
    "import knifey\n",
    "knifey.maybe_download_and_extract()\n",
    "knifey.copy_files()\n",
    "train_dir = knifey.train_dir\n",
    "test_dir = knifey.test_dir'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### fine-tune model..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Data generation / parameters randomizaton...\n",
    "Still unsure if such pre-processing for the frames will ultimately help our model. How would we go about it?\n",
    "\n",
    "Ideas:\n",
    "    - Every video input must be excluded/separated\n",
    "    - preprocessing won't be for each frame, but instead for each set of frames that correspond to a full video. Basically the video will be proprocessed and not individual frames.\n",
    "        \n",
    "###### Functions to (randomly) change image parameters\n",
    "\n",
    "###### Not sure if this will help, unless we have a lot of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"train_datagen = ImageDataGenerator(rescale=1. / 255,\\n                   rotation_range=transformation_ratio,\\n                   shear_range=transformation_ratio,\\n                   zoom_range=transformation_ratio,\\n                   cval=transformation_ratio,\\n                   horizontal_flip=True,\\n                   vertical_flip=True)\\n\\nvalidation_datagen = ImageDataGenerator(rescale=1. / 255)\\n\\nvalidation_generator = validation_datagen.flow_from_directory(validation_data_dir,\\n                          target_size=(img_width, img_height),\\n                          batch_size=batch_size,\\n                          class_mode='categorical')\\n\\n\\n# Creates new directory if it does not exist, in the joined path of the train_data_dir path\\n\\nos.makedirs(os.path.join(os.path.abspath(train_data_dir), '../preview'), exist_ok=True)\\n\\n\\n# the data generator takes in:\\n    # The directory of the data\\n    # gets a small batch size of files\\n    # resizes them to the target_size\\n# it spits out a batch of images with different parameters\\n\\ntrain_generator = train_datagen.flow_from_directory(train_data_dir,\\n                    target_size=(img_width, img_height),\\n                    batch_size=batch_size,\\n                    class_mode='categorical')\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# functions that will be used to change the image parameters for randomness\n",
    "# Note: the validation data generator only rescales the images for obvious reasons\n",
    "'''train_datagen = ImageDataGenerator(rescale=1. / 255,\n",
    "                   rotation_range=transformation_ratio,\n",
    "                   shear_range=transformation_ratio,\n",
    "                   zoom_range=transformation_ratio,\n",
    "                   cval=transformation_ratio,\n",
    "                   horizontal_flip=True,\n",
    "                   vertical_flip=True)\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(validation_data_dir,\n",
    "                          target_size=(img_width, img_height),\n",
    "                          batch_size=batch_size,\n",
    "                          class_mode='categorical')\n",
    "\n",
    "\n",
    "# Creates new directory if it does not exist, in the joined path of the train_data_dir path\n",
    "\n",
    "os.makedirs(os.path.join(os.path.abspath(train_data_dir), '../preview'), exist_ok=True)\n",
    "\n",
    "\n",
    "# the data generator takes in:\n",
    "    # The directory of the data\n",
    "    # gets a small batch size of files\n",
    "    # resizes them to the target_size\n",
    "# it spits out a batch of images with different parameters\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(train_data_dir,\n",
    "                    target_size=(img_width, img_height),\n",
    "                    batch_size=batch_size,\n",
    "                    class_mode='categorical')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet50 additional resources\n",
    "\n",
    "* Deep Residual Networks https://github.com/KaimingHe/deep-residual-networks\n",
    "* Graph: http://ethereon.github.io/netscope/#/gist/db945b393d40bfa26006\n",
    "* Keras ResNet50 Implementation https://github.com/raghakot/keras-resnet\n",
    "* https://www.pyimagesearch.com/2017/03/20/imagenet-vggnet-resnet-inception-xception-keras/\n",
    "* https://www.quora.com/What-is-the-deep-neural-network-known-as-%E2%80%9CResNet-50%E2%80%9D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "* ffmpeg to convert videos to frames\n",
    "* Fine tuning of top layers (in keras.io website)\n",
    "* 24-30 frames per second is eye friendly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pickling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import dill\\n\\n# Save session\\ndill.dump_session('saved_sessions/testPickle.db')\\n\\n# Load session\\n# dill.load_session('saved_sessions/testPickle.db')\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import dill\n",
    "\n",
    "# Save session\n",
    "dill.dump_session('saved_sessions/testPickle.db')\n",
    "\n",
    "# Load session\n",
    "# dill.load_session('saved_sessions/testPickle.db')'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
